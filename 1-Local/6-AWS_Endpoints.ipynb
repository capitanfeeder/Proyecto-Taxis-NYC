{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Funciones para Endpoints en AWS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Elegimos realizar nuestras operaciones de Big Data con [`AWS`](https://aws.amazon.com/es/) debido a que es la empresa líder en el sector y cuenta con una alta demanda en la industria. La decisión se basa en la reputación y experiencia sobresalientes de [`AWS`](https://aws.amazon.com/es/) en la gestión eficiente de datos a gran escala. Optar por [`AWS`](https://aws.amazon.com/es/) nos permite aprovechar su infraestructura sólida, servicios avanzados y la confianza que inspira en el mercado, asegurando así que nuestras operaciones de Big Data estén respaldadas por una plataforma de clase mundial reconocida por su fiabilidad y eficacia.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos las librerías necesarias para el desarrollo de las funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.fs\n",
    "from s3fs import S3FileSystem\n",
    "import s3fs\n",
    "import joblib\n",
    "from fastapi import FastAPI, Depends\n",
    "from pydantic import BaseModel\n",
    "from datetime import timedelta\n",
    "from pyathena import connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probamos la conexión con el bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo: air_quality.parquet, Tamaño: 4692 bytes\n",
      "Archivo: climate_NY.parquet, Tamaño: 14001 bytes\n",
      "Archivo: electric_and_alternative_fuel_charging_stations.parquet, Tamaño: 14938 bytes\n",
      "Archivo: fuel_prices.parquet, Tamaño: 2498 bytes\n",
      "Archivo: sound_quality.parquet, Tamaño: 9825 bytes\n",
      "Archivo: taxi_zones.parquet, Tamaño: 7501 bytes\n",
      "Archivo: taxis_2023.parquet, Tamaño: 515237183 bytes\n",
      "Archivo: vehicles_info.parquet, Tamaño: 9813 bytes\n",
      "Archivo: xgb_model.pkl, Tamaño: 422934 bytes\n"
     ]
    }
   ],
   "source": [
    "# Especificamos el nombre de tu bucket y el identificador de la región de AWS\n",
    "bucket_name = 'tu_bucket'\n",
    "region_name = 'tu_region'\n",
    "aws_access_key_id = 'tu_access_key_id'\n",
    "aws_secret_access_key = 'tu_secret_access_key'\n",
    "\n",
    "# Creamos un cliente de S3 con las credenciales proporcionadas\n",
    "s3 = boto3.client('s3', region_name=region_name, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "# Listamos objetos en el bucket\n",
    "response = s3.list_objects(Bucket=bucket_name)\n",
    "for obj in response.get('Contents', []):\n",
    "    print(f'Archivo: {obj[\"Key\"]}, Tamaño: {obj[\"Size\"]} bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisamos el contenido de un archivo del bucket al azar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74 entries, 0 to 73\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   Year                 74 non-null     int64 \n",
      " 1   Manufacturer         74 non-null     object\n",
      " 2   Model                74 non-null     object\n",
      " 3   Fuel                 74 non-null     object\n",
      " 4   Alternative Fuel     74 non-null     object\n",
      " 5   Miles per gallon     74 non-null     int64 \n",
      " 6   CO2 Emission (g/mi)  74 non-null     int64 \n",
      " 7   Sound Emission (dB)  74 non-null     int64 \n",
      " 8   Price (USD)          74 non-null     int64 \n",
      " 9   Range (mi)           74 non-null     int64 \n",
      " 10  Category             74 non-null     object\n",
      "dtypes: int64(6), object(5)\n",
      "memory usage: 6.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Especificamos las credenciales de AWS\n",
    "aws_access_key_id = 'tu_access_key_id'\n",
    "aws_secret_access_key = 'tu_secret_access_key'\n",
    "\n",
    "# Especificamos la ruta de tu archivo Parquet en S3\n",
    "s3_path = 'tu_bucket/tu_archivo'\n",
    "\n",
    "# Configuramos la conexión a S3 con las credenciales\n",
    "s3 = s3fs.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)\n",
    "\n",
    "# Abrimos el archivo Parquet directamente desde S3\n",
    "parquet_file = pq.ParquetFile(s3.open(s3_path))\n",
    "\n",
    "# Leemos el archivo Parquet en un DataFrame de pandas\n",
    "df = parquet_file.read().to_pandas()\n",
    "\n",
    "# Imprimimos los resultados\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Función que devuelve la predicción porcentual de oportunidad de viajes de un usuario.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputData(BaseModel):\n",
    "    pBoroughID: int\n",
    "    dayofweek: int\n",
    "    hour: int\n",
    "\n",
    "\n",
    "def predict(data: InputData):\n",
    "\n",
    "    # Especificamos las credenciales de AWS\n",
    "    aws_access_key_id = 'tu_access_key_id'\n",
    "    aws_secret_access_key = 'tu_secret_access_key'\n",
    "\n",
    "    # Configuramos la conexión a S3 con las credenciales\n",
    "    s3 = s3fs.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)\n",
    "\n",
    "    # Especificamos la ruta del modelo en S3\n",
    "    model_path = 'tu_bucket/tu_modelo'\n",
    "\n",
    "    # Descargamos el modelo desde S3\n",
    "    with s3.open(model_path, 'rb') as f:\n",
    "        xgb_model = joblib.load(f)\n",
    "\n",
    "    # Convertirmos los datos de entrada a un DataFrame\n",
    "    input_data = pd.DataFrame([data.model_dump()])\n",
    "\n",
    "    # Realizarmos la predicción\n",
    "    prediction = xgb_model.predict(input_data)\n",
    "\n",
    "    # Devolvemos la predicción como JSON\n",
    "    return {\"Probabilidad de conseguir pasajero\": prediction.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Probabilidad de conseguir pasajero': [79.81958770751953]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = InputData(pBoroughID=1, dayofweek=5, hour=18)\n",
    "predict(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Función que devuelve un top 3 de vehículos ecológicos de acuerdo al capital ingresado`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_3_vehicles(max_price_usd: float):\n",
    "\n",
    "    # Especificamos las credenciales de AWS\n",
    "    aws_access_key_id = 'tu_access_key_id'\n",
    "    aws_secret_access_key = 'tu_secret_access_key'\n",
    "\n",
    "    # Especificamos la ruta del archivo Parquet en S3\n",
    "    s3_path = 'tu_bucket/tu_archivo'\n",
    "\n",
    "    # Configura la conexión a S3 con las credenciales\n",
    "    s3 = s3fs.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)\n",
    "\n",
    "    # Abrimos el archivo Parquet directamente desde S3\n",
    "    parquet_file = pq.ParquetFile(s3.open(s3_path))\n",
    "\n",
    "    # Leemos el archivo Parquet en un DataFrame de pandas\n",
    "    df = parquet_file.read().to_pandas()\n",
    "\n",
    "    # Filtramos los vehículos por precio\n",
    "    filtered_vehicles = df[df['Price (USD)'] <= max_price_usd]\n",
    "\n",
    "    # Ordenamos los vehículos por CO2 en orden ascendente, luego por Sound Emission y finalmente por Range\n",
    "    sorted_vehicles = filtered_vehicles.sort_values(by=['CO2 Emission (g/mi)', 'Sound Emission (dB)', 'Range (mi)'], ascending=[True, True, False])\n",
    "\n",
    "    # Tomamos los tres primeros vehículos dentro del rango de precio\n",
    "    top_3_vehicles = sorted_vehicles.head(3)\n",
    "\n",
    "    # Creamos el formato de salida como una lista de diccionarios\n",
    "    output_format = []\n",
    "    for idx, row in enumerate(top_3_vehicles.itertuples(), start=1):\n",
    "        output_format.append({\n",
    "            f'Puesto {idx}': f'{row.Manufacturer} {row.Model}',\n",
    "            'Precio (USD)': row._9,\n",
    "            'Combustible': row.Fuel,\n",
    "            'CO2': row._7,\n",
    "            'dB': row._8,\n",
    "            'Millas con un tanque lleno': row._10,\n",
    "        })\n",
    "\n",
    "    return output_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Puesto 1': 'Chevrolet Equinox Hybrid',\n",
       "  'Precio (USD)': 24995,\n",
       "  'Combustible': 'Gasoline',\n",
       "  'CO2': 25,\n",
       "  'dB': 68,\n",
       "  'Millas con un tanque lleno': 440},\n",
       " {'Puesto 2': 'Toyota Prius',\n",
       "  'Precio (USD)': 25295,\n",
       "  'Combustible': 'Gasoline',\n",
       "  'CO2': 41,\n",
       "  'dB': 65,\n",
       "  'Millas con un tanque lleno': 54},\n",
       " {'Puesto 3': 'Toyota Prius',\n",
       "  'Precio (USD)': 24525,\n",
       "  'Combustible': 'Gasoline',\n",
       "  'CO2': 41,\n",
       "  'dB': 68,\n",
       "  'Millas con un tanque lleno': 520}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_3_vehicles(25500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Función que devuelve estadísticas de viajes de acuerdo al Borough ingresado`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxi_stats_test(pickup_borough: str):\n",
    "    # Configuramos las credenciales de AWS\n",
    "    aws_access_key_id = 'tu_access_key_id'\n",
    "    aws_secret_access_key = 'tu_secret_access_key'\n",
    "    region_name = 'tu_region'\n",
    "\n",
    "    # Configuramos la conexión a Athena\n",
    "    conn = connect(aws_access_key_id=aws_access_key_id,\n",
    "                aws_secret_access_key=aws_secret_access_key,\n",
    "                region_name=region_name,\n",
    "                schema_name='tu_schema',\n",
    "                s3_staging_dir='tu_bucket')\n",
    "\n",
    "    # Ejecutamos una consulta\n",
    "    query = '''\n",
    "        WITH tu_table AS (\n",
    "    SELECT\n",
    "        \"pickup_borough\",\n",
    "        COUNT(*) AS \"Viajes_Totales\",\n",
    "        ROUND(AVG(date_diff('second', \"pickup_datetime\", \"dropoff_datetime\")) / 3600, 2) AS \"Duracion_Promedio_Hs\",\n",
    "        ROUND(COUNT(*) / COUNT(DISTINCT date_trunc('day', \"pickup_datetime\")), 2) AS \"Media_de_Viajes_por_Dia\",\n",
    "        ROUND(COUNT(*) / COUNT(DISTINCT date_trunc('month', \"pickup_datetime\")), 2) AS \"Media_de_Viajes_por_Mes\",\n",
    "        ROUND(AVG(\"trip_distance\"), 2) AS \"Distancia_recorrida_promedio_millas\",\n",
    "        ROUND(AVG(\"total_amount\"), 2) AS \"Total_ganado_en_promedio_USD\"\n",
    "    FROM\n",
    "        \"tu_database\".\"tu_table\"\n",
    "    GROUP BY\n",
    "        \"pickup_borough\"\n",
    "    )\n",
    "    SELECT\n",
    "    *\n",
    "    FROM\n",
    "    tu_table\n",
    "    ORDER BY\n",
    "    \"pickup_borough\"\n",
    "    '''\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query, parameters={'pickup_borough': 'Manhattan'})  \n",
    "\n",
    "    result = pd.DataFrame(cursor.fetchall(), columns=[\"pickup_borough\", \"Viajes_Totales\", \"Duracion_Promedio_Hs\", \"Media_de_Viajes_por_Dia\", \"Media_de_Viajes_por_Mes\", \"Distancia_recorrida_promedio_millas\", \"Total_ganado_en_promedio_USD\"])\n",
    "\n",
    "    borough_result = result[result[\"pickup_borough\"] == pickup_borough]\n",
    "\n",
    "    formatted_result = {\n",
    "        \"Viajes Totales\": borough_result[\"Viajes_Totales\"].values[0],\n",
    "        \"Duración Promedio (Hs)\": borough_result[\"Duracion_Promedio_Hs\"].values[0],\n",
    "        \"Media de Viajes por Día\": borough_result[\"Media_de_Viajes_por_Dia\"].values[0],\n",
    "        \"Media de Viajes por Mes\": borough_result[\"Media_de_Viajes_por_Mes\"].values[0],\n",
    "        \"Distancia recorrida promedio (millas)\": borough_result[\"Distancia_recorrida_promedio_millas\"].values[0],\n",
    "        \"Total ganado en promedio (USD)\": borough_result[\"Total_ganado_en_promedio_USD\"].values[0]\n",
    "    }\n",
    "\n",
    "    # En el entorno de Athena, enfrentamos un inconveniente al realizar consultas desde Python, ya que dichas consultas generan\n",
    "    # automáticamente dos archivos en el bucket: uno con extensión .csv y otro con extensión .metadata. Esta situación impide la\n",
    "    # ejecución de nuevas consultas hasta que ambos archivos sean eliminados. Para superar este problema, hemos implementado una\n",
    "    # solución que conecta con S3 añadiendo las siguientes líneas de código. Estas líneas permiten la eliminación de los archivos generados,\n",
    "    # posibilitando así la ejecución de consultas sin inconvenientes.\n",
    "    \n",
    "    s3_client = boto3.client('s3',\n",
    "                            aws_access_key_id=aws_access_key_id,\n",
    "                            aws_secret_access_key=aws_secret_access_key,\n",
    "                            region_name=region_name)\n",
    "\n",
    "    # Definimos el nombre del bucket\n",
    "    bucket_name = 'tu_bucket'\n",
    "\n",
    "    # Obtenemos la lista de objetos en el bucket\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "\n",
    "    # Eliminamos los archivos .csv y .metadata encontrados\n",
    "    for obj in response['Contents']:\n",
    "        if obj['Key'].endswith('.csv') or obj['Key'].endswith('.metadata'):\n",
    "            s3_client.delete_object(Bucket=bucket_name, Key=obj['Key'])\n",
    "\n",
    "\n",
    "    return formatted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Viajes Totales': 24235349,\n",
       " 'Duración Promedio (Hs)': 0.25,\n",
       " 'Media de Viajes por Día': 85940,\n",
       " 'Media de Viajes por Mes': 1425608,\n",
       " 'Distancia recorrida promedio (millas)': 3.24,\n",
       " 'Total ganado en promedio (USD)': 26.35}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_stats_test('Manhattan')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
