{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implementación de servicios de Amazon Web Services**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En esta primer estapa, creamos una función `Lambda` que se encarga de realizar web scrapping a la página [Taxi & Limousine Comission](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) para extraer los datos de los viajes realizados en la ciudad de New York durante el año 2023 y finalmente almacenarlos en un bucket de [AWS S3](https://aws.amazon.com/es/s3/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Definimos la función de manejo de la solicitud\n",
    "def lambda_handler(event, context):\n",
    "    # Configuramos las credenciales de AWS\n",
    "    AWS_ACCESS_KEY_ID = 'tu_access_key'\n",
    "    AWS_SECRET_ACCESS_KEY = 'tu_secret_key'\n",
    "    AWS_REGION = 'tu_region'\n",
    "\n",
    "    # Creamos un cliente de S3\n",
    "    s3_client = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, region_name=AWS_REGION)\n",
    "\n",
    "    # Guardamos la URL en una variable\n",
    "    url = 'https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page'\n",
    "\n",
    "    # Realizamos la solicitud HTTP\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Verificamos si la solicitud fue exitosa\n",
    "    if response.status_code == 200:\n",
    "        # Analizamos el contenido HTML con BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Buscamos el elemento <div> con la clase \"faq-answers\" y el id \"faq2023\"\n",
    "        target_div = soup.find('div', {'class': 'faq-answers', 'id': 'faq2023'})\n",
    "\n",
    "        if target_div:\n",
    "            # Buscamos los enlaces <a> dentro de target_div que tienen title=\"Yellow Taxi Trip Records\" o \"Green Taxi Trip Records\"\n",
    "            taxi_links = target_div.find_all('a', title=['Yellow Taxi Trip Records', 'Green Taxi Trip Records'])\n",
    "\n",
    "            if taxi_links:\n",
    "                print(\"Enlaces con el título 'Yellow Taxi Trip Records' o 'Green Taxi Trip Records':\")\n",
    "                links = []\n",
    "                for link in taxi_links:\n",
    "                    links.append(link.get('href'))\n",
    "                    print(link.get('href'))  # Imprime el enlace URL\n",
    "            else:\n",
    "                print(\"No se encontraron enlaces con los títulos 'Yellow Taxi Trip Records' o 'Green Taxi Trip Records' dentro de 'faq-answers'.\")\n",
    "        else:\n",
    "            print(\"No se encontró el elemento con la clase 'faq-answers' y el id 'faq2023'.\")\n",
    "    else:\n",
    "        print(\"La solicitud no fue exitosa. Código de estado:\", response.status_code)\n",
    "\n",
    "    # Generamos dinámicamente los enlaces para los meses del 01 al 10\n",
    "    meses = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "\n",
    "    # Guardamos la URL en una variable\n",
    "    url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "    # Realizamos la solicitud HTTP a la página\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Verificamos si la solicitud fue exitosa (código de estado 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parseamos el contenido HTML de la página\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Encontramos todos los enlaces que contienen 'yellow' o 'green' y tienen el formato esperado para 2023\n",
    "        links_pagina = {a['href'] for a in soup.find_all('a', href=True) if re.match(r'^.*(yellow|green)_tripdata_2023-\\d{2}\\.parquet$', a['href'])}\n",
    "\n",
    "        # Combinamos enlaces generados dinámicamente con los obtenidos de la página\n",
    "        links_totales = links_pagina.copy()  # Creamos una copia para mantener los enlaces originales\n",
    "        for mes in meses:\n",
    "            enlace_yellow = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-{mes}.parquet\"\n",
    "            enlace_green = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-{mes}.parquet\"\n",
    "            links_totales.add(enlace_yellow)\n",
    "            links_totales.add(enlace_green)\n",
    "\n",
    "        # Realizamos la descarga de los archivos y los subimos a S3\n",
    "        for link in sorted(links_totales):\n",
    "            nombre_archivo = link.split(\"/\")[-1]  # Obtenemos el nombre del archivo desde la URL\n",
    "\n",
    "            try:\n",
    "                # Verificamos si el archivo ya existe en el bucket\n",
    "                s3_client.head_object(Bucket='tu_bucket', Key=nombre_archivo)\n",
    "                print(f\"El archivo {nombre_archivo} ya existe en S3.\")\n",
    "            except:\n",
    "                response = requests.get(link)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    # Subimos el archivo directamente a S3\n",
    "                    try:\n",
    "                        s3_client.put_object(Body=response.content, Bucket='tu_bucket', Key=nombre_archivo)\n",
    "                        print(f\"Archivo subido a S3: {nombre_archivo}\")\n",
    "                    except NoCredentialsError:\n",
    "                        print('Las credenciales de AWS no están disponibles.')\n",
    "                else:\n",
    "                    print(f\"No se pudo descargar el archivo: {nombre_archivo}. Código de estado: {response.status_code}\")\n",
    "    else:\n",
    "        print(f\"No se pudo acceder a la página. Código de estado: {response.status_code}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lambda_handler(None, None)  # Ejecutamos la función lambda_handler localmente para pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una vez obtenidos los archivos parquet, recurrimos al uso de [AWS Glue](https://aws.amazon.com/es/glue/) para automatizar el proceso de normalización de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, udf\n",
    "from pyspark.sql.functions import expr, when\n",
    "\n",
    "\n",
    "# Creamos un SparkSession\n",
    "spark = SparkSession.builder.appName(\"Concatenate Parquet Files\").getOrCreate()\n",
    "\n",
    "\n",
    "# TAXIS VERDES\n",
    "\n",
    "\n",
    "# Creamos la lista para almacenar los DataFrames (green)\n",
    "df_green_list = []\n",
    "\n",
    "# Iteramos sobre los meses\n",
    "for month in range(1, 11):\n",
    "\n",
    "    # Obtenemos la ruta al archivo Parquet (green)\n",
    "    parquet_file_green_path = \"s3://tu_bucket_de_entrada/green_tripdata_2023-{0}.parquet\".format(str(month).zfill(2))\n",
    "\n",
    "    # Leemos el archivo Parquet (green)\n",
    "    df_green = spark.read.parquet(parquet_file_green_path)\n",
    "\n",
    "    # Agregamos el DataFrame a la lista (green)\n",
    "    df_green_list.append(df_green)\n",
    "\n",
    "# Concatenamos los DataFrames de la lista (green)\n",
    "concatenated_df_green = df_green_list[0]  # Iniciamos la concatenación con el primer DataFrame\n",
    "for i in range(1, len(df_green_list)):\n",
    "    concatenated_df_green = concatenated_df_green.union(df_green_list[i])  # Usamos el método union de DataFrames\n",
    "\n",
    "# Eliminamos columnas\n",
    "columns_to_drop_green = ['store_and_fwd_flag', 'fare_amount', 'extra', 'mta_tax', 'tolls_amount', 'improvement_surcharge',\n",
    "                         'ehail_fee', 'congestion_surcharge', 'trip_type', 'VendorID', 'RatecodeID', 'payment_type']\n",
    "concatenated_df_green = concatenated_df_green.drop(*columns_to_drop_green)\n",
    "\n",
    "# Renombramos columnas\n",
    "column_rename_green = {\n",
    "    'lpep_pickup_datetime': 'pickup_datetime',\n",
    "    'lpep_dropoff_datetime': 'dropoff_datetime',\n",
    "    'total_amount': 'amount'\n",
    "}\n",
    "for old_col, new_col in column_rename_green.items():\n",
    "    concatenated_df_green = concatenated_df_green.withColumnRenamed(old_col, new_col)\n",
    "    \n",
    "# Sumamos las columnas amount y tip_amount para crear la columna total_amount\n",
    "concatenated_df_green = concatenated_df_green.withColumn('total_amount', concatenated_df_green['amount'] + concatenated_df_green['tip_amount'])\n",
    "\n",
    "# Eliminamos las columnas amount y tip_amount al no ser necesarias\n",
    "columns_to_drop_green = ['amount', 'tip_amount']\n",
    "concatenated_df_green = concatenated_df_green.drop(*columns_to_drop_green)\n",
    "\n",
    "# Agregamos la columna 'service_type'\n",
    "concatenated_df_green = concatenated_df_green.withColumn('service_type', F.lit('green'))\n",
    "\n",
    "# Reducimos a una sola partición antes de escribir (green)\n",
    "concatenated_df_green = concatenated_df_green.coalesce(1)\n",
    "\n",
    "\n",
    "# TAXIS AMARILLOS\n",
    "\n",
    "\n",
    "# Creamos la lista para almacenar los DataFrames (yellow)\n",
    "df_yellow_list = []\n",
    "\n",
    "# Iteramos sobre los meses\n",
    "for month in range(1, 11):\n",
    "\n",
    "    # Obtenemos la ruta al archivo Parquet (yellow)\n",
    "    parquet_file_yellow_path = \"s3://tu_bucket_de_entrada/yellow_tripdata_2023-{0}.parquet\".format(str(month).zfill(2))\n",
    "\n",
    "    # Leemos el archivo Parquet (yellow)\n",
    "    df_yellow = spark.read.parquet(parquet_file_yellow_path)\n",
    "\n",
    "    # Agregamos el DataFrame a la lista (yellow)\n",
    "    df_yellow_list.append(df_yellow)\n",
    "\n",
    "# Concatenamos los DataFrames de la lista (yellow)\n",
    "concatenated_df_yellow = df_yellow_list[0]  # Iniciamos la concatenación con el primer DataFrame\n",
    "for i in range(1, len(df_yellow_list)):\n",
    "    concatenated_df_yellow = concatenated_df_yellow.union(df_yellow_list[i])  # Usamos el método union de DataFrames\n",
    "\n",
    "# Eliminamos columnas\n",
    "columns_to_drop_yellow = ['store_and_fwd_flag', 'fare_amount', 'extra', 'mta_tax', 'tolls_amount', 'improvement_surcharge',\n",
    "                           'congestion_surcharge', 'airport_fee', 'Airport_fee', 'VendorID', 'RatecodeID', 'payment_type']\n",
    "concatenated_df_yellow = concatenated_df_yellow.drop(*columns_to_drop_yellow)\n",
    "\n",
    "# Renombramos columnas\n",
    "column_rename_yellow = {\n",
    "    'tpep_pickup_datetime': 'pickup_datetime',\n",
    "    'tpep_dropoff_datetime': 'dropoff_datetime',\n",
    "    'total_amount': 'amount'\n",
    "}\n",
    "for old_col, new_col in column_rename_yellow.items():\n",
    "    concatenated_df_yellow = concatenated_df_yellow.withColumnRenamed(old_col, new_col)\n",
    "    \n",
    "# Sumamos las columnas amount y tip_amount para crear la columna total_amount\n",
    "concatenated_df_yellow = concatenated_df_yellow.withColumn('total_amount', concatenated_df_yellow['amount'] + concatenated_df_yellow['tip_amount'])\n",
    "\n",
    "# Eliminamos las columnas amount y tip_amount al no ser necesarias\n",
    "columns_to_drop_yellow = ['amount', 'tip_amount']\n",
    "concatenated_df_yellow = concatenated_df_yellow.drop(*columns_to_drop_yellow)\n",
    "\n",
    "# Agregamos la columna 'service_type'\n",
    "concatenated_df_yellow = concatenated_df_yellow.withColumn('service_type', F.lit('yellow'))\n",
    "\n",
    "# Reducimos a una sola partición antes de escribir (yellow)\n",
    "concatenated_df_yellow = concatenated_df_yellow.coalesce(1)\n",
    "\n",
    "# Reorganizamos columnas para taxis amarillos\n",
    "column_order_yellow = concatenated_df_green.columns\n",
    "concatenated_df_yellow = concatenated_df_yellow.select(*column_order_yellow)\n",
    "\n",
    "\n",
    "# CONCATENAMOS TAXIS VERDES Y AMARILLOS\n",
    "\n",
    "\n",
    "# Fusionamos los DataFrames green y yellow\n",
    "concatenated_taxis = concatenated_df_green.union(concatenated_df_yellow)\n",
    "\n",
    "# Reducimos a una sola partición antes de escribir (opcional)\n",
    "concatenated_taxis = concatenated_taxis.coalesce(1)\n",
    "\n",
    "# Cambiamos el tipo de dato de 'passenger_count' a IntegerType\n",
    "concatenated_taxis = concatenated_taxis.withColumn('passenger_count', concatenated_taxis['passenger_count'].cast('int'))\n",
    "\n",
    "# Reemplazamos los valores nulos de 'Passenger_count' por 1\n",
    "concatenated_taxis = concatenated_taxis.na.fill({'passenger_count': 1})\n",
    "\n",
    "# Reemplazamos los valores 0.00 en 'trip_distance' por 1.00\n",
    "concatenated_taxis = concatenated_taxis.withColumn('trip_distance', F.when(concatenated_taxis['trip_distance'] == 0.00, 1.00).otherwise(concatenated_taxis['trip_distance']))\n",
    "\n",
    "# Creamos columnas 'pickup_borough' y 'dropoff_borough' usando expresiones condicionales\n",
    "pickup_borough_expr = expr(\"\"\"\n",
    "    CASE\n",
    "        WHEN PULocationID IN (4, 12, 13, 24, 41, 42, 43, 45, 48, 50, 68, 74, 75, 79, 87, 88, 90, 100, 103, 103, 103, 107, 113, 114, 116, 120, 125,\n",
    "                             127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153, 158, 161, 162, 163, 164, 166, 170, 186, 194, 202, 209, 211,\n",
    "                             224, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 243, 244, 246, 249, 261, 262, 263) THEN 'Manhattan'\n",
    "        WHEN PULocationID IN (11, 14, 17, 21, 22, 25, 26, 29, 33, 34, 35, 36, 37, 39, 40, 49, 52, 54, 55, 61, 62, 63, 65, 66, 67, 71, 72, 76, 77, 80,\n",
    "                             85, 89, 91, 97, 106, 108, 111, 112, 123, 133, 149, 150, 154, 155, 165, 177, 178, 181, 188, 189, 190, 195, 210, 217, 222,\n",
    "                             225, 227, 228, 255, 256, 257) THEN 'Brooklyn'\n",
    "        WHEN PULocationID IN (2, 7, 8, 9, 10, 15, 16, 19, 27, 28, 30, 38, 53, 56, 56, 64, 70, 73, 82, 83, 86, 92, 93, 95, 96, 98, 101, 102, 117, 121,\n",
    "                              122, 124, 129, 130, 131, 132, 134, 135, 138, 139, 145, 146, 157, 160, 171, 173, 175, 179, 180, 191, 192, 193, 196, 197,\n",
    "                              198, 201, 203, 205, 207, 215, 216, 218, 219, 223, 226, 252, 253, 258, 260) THEN 'Queens'\n",
    "        WHEN PULocationID IN (3, 18, 20, 31, 32, 46, 47, 51, 58, 59, 60, 69, 78, 81, 94, 119, 126, 136, 147, 159, 167, 168, 169, 174, 182, 183, 184,\n",
    "                              185, 199, 200, 208, 212, 213, 220, 235, 240, 241, 242, 247, 248, 250, 254, 259) THEN 'Bronx'\n",
    "        WHEN PULocationID IN (5, 6, 23, 44, 84, 99, 109, 110, 115, 118, 156, 172, 176, 187, 204, 206, 214, 221, 245, 251) THEN 'Staten Island'\n",
    "        WHEN PULocationID IN (1) THEN 'EWR'\n",
    "        ELSE 'Unknown'\n",
    "    END AS pickup_borough\n",
    "\"\"\")\n",
    "\n",
    "dropoff_borough_expr = expr(\"\"\"\n",
    "    CASE\n",
    "        WHEN DOLocationID IN (4, 12, 13, 24, 41, 42, 43, 45, 48, 50, 68, 74, 75, 79, 87, 88, 90, 100, 103, 103, 103, 107, 113, 114, 116, 120, 125,\n",
    "                             127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153, 158, 161, 162, 163, 164, 166, 170, 186, 194, 202, 209, 211,\n",
    "                             224, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 243, 244, 246, 249, 261, 262, 263) THEN 'Manhattan'\n",
    "        WHEN DOLocationID IN (11, 14, 17, 21, 22, 25, 26, 29, 33, 34, 35, 36, 37, 39, 40, 49, 52, 54, 55, 61, 62, 63, 65, 66, 67, 71, 72, 76, 77, 80,\n",
    "                             85, 89, 91, 97, 106, 108, 111, 112, 123, 133, 149, 150, 154, 155, 165, 177, 178, 181, 188, 189, 190, 195, 210, 217, 222,\n",
    "                             225, 227, 228, 255, 256, 257) THEN 'Brooklyn'\n",
    "        WHEN DOLocationID IN (2, 7, 8, 9, 10, 15, 16, 19, 27, 28, 30, 38, 53, 56, 56, 64, 70, 73, 82, 83, 86, 92, 93, 95, 96, 98, 101, 102, 117, 121,\n",
    "                              122, 124, 129, 130, 131, 132, 134, 135, 138, 139, 145, 146, 157, 160, 171, 173, 175, 179, 180, 191, 192, 193, 196, 197,\n",
    "                              198, 201, 203, 205, 207, 215, 216, 218, 219, 223, 226, 252, 253, 258, 260) THEN 'Queens'\n",
    "        WHEN DOLocationID IN (3, 18, 20, 31, 32, 46, 47, 51, 58, 59, 60, 69, 78, 81, 94, 119, 126, 136, 147, 159, 167, 168, 169, 174, 182, 183, 184,\n",
    "                              185, 199, 200, 208, 212, 213, 220, 235, 240, 241, 242, 247, 248, 250, 254, 259) THEN 'Bronx'\n",
    "        WHEN DOLocationID IN (5, 6, 23, 44, 84, 99, 109, 110, 115, 118, 156, 172, 176, 187, 204, 206, 214, 221, 245, 251) THEN 'Staten Island'\n",
    "        WHEN DOLocationID IN (1) THEN 'EWR'\n",
    "        ELSE 'Unknown'\n",
    "    END AS dropoff_borough\n",
    "\"\"\")\n",
    "\n",
    "concatenated_taxis = concatenated_taxis.withColumn('pickup_borough', pickup_borough_expr)\n",
    "concatenated_taxis = concatenated_taxis.withColumn('dropoff_borough', dropoff_borough_expr)\n",
    "\n",
    "# Eliminamos las filas donde 'pickup_borough' o 'dropoff_borough' sean 'Unknown'\n",
    "concatenated_taxis = concatenated_taxis.filter(concatenated_taxis.pickup_borough != 'Unknown')\n",
    "\n",
    "# Guardamos el DataFrame concatenado\n",
    "concatenated_taxis.write.parquet(\"s3://tu_bucket_de_salida/tu_carpeta\")\n",
    "\n",
    "# Detenemos la sesión de Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completada la normalización de los datos, se guarda el archivo resultante en formato Parquet en un bucket de [AWS S3](https://aws.amazon.com/es/s3/). Sin embargo, el archivo se guarda con el nombre predeterminado `part-00000-8bbda55c-fad7-46b5-9f8d-b8d8e336c149-c000.snappy.parquet`. Este nombre se genera automáticamente cuando se guarda un archivo Parquet utilizando Apache Spark con el formato de compresión Snappy.\n",
    "### Para solucionar este problema, utilizamos otra función `Lambda` para renombrar el archivo y almacenarlo en un bucket que alimenta a nuestro `Data Warehouse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Nombre del bucket y ruta del archivo original\n",
    "    source_bucket_name = 'tu_bucket_de_entrada'\n",
    "    destination_bucket_name = 'tu_bucket_de_salida'\n",
    "    source_folder_name = 'tu_carpeta/'\n",
    "    \n",
    "    # Creamos una instancia del cliente de S3 con tus credenciales de acceso\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        region_name='tu_region',\n",
    "        aws_access_key_id='tu_access_key',\n",
    "        aws_secret_access_key='tu_secret_key'\n",
    "    )\n",
    "    \n",
    "    # Obtenemos la lista de objetos en la carpeta especificada\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=source_bucket_name,\n",
    "        Prefix=source_folder_name\n",
    "    )\n",
    "    \n",
    "    # Renombramos y copiamos cada archivo que termine en \".parquet\" al nuevo bucket\n",
    "    for obj in response.get('Contents', []):\n",
    "        file_key = obj['Key']\n",
    "        if file_key.endswith('.parquet'):\n",
    "            new_file_key = 'nuevo_nombre.parquet'\n",
    "            s3_client.copy_object(\n",
    "                Bucket=destination_bucket_name,\n",
    "                CopySource={'Bucket': source_bucket_name, 'Key': file_key},\n",
    "                Key=new_file_key\n",
    "            )\n",
    "            s3_client.delete_object(\n",
    "                Bucket=source_bucket_name,\n",
    "                Key=file_key\n",
    "            )\n",
    "    \n",
    "    # Eliminamos todos los objetos dentro de la carpeta original\n",
    "    for obj in response.get('Contents', []):\n",
    "        s3_client.delete_object(\n",
    "            Bucket=source_bucket_name,\n",
    "            Key=obj['Key']\n",
    "        )\n",
    "\n",
    "    # Eliminamos la carpeta original después de copiar y eliminamos todos los archivos\n",
    "    s3_client.delete_object(\n",
    "        Bucket=source_bucket_name,\n",
    "        Key=source_folder_name\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': 'Archivos renombrados, copiados al nuevo bucket y carpeta eliminada exitosamente'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una vez que el archivo se encuentra en el bucket que nutre a [AWS Athena](https://aws.amazon.com/es/athena/), podemos realizar consultas SQL para obtener información de los datos almacenados en el `Data Warehouse` como así también utilizar la librería PyAthena para hacer consultas desde Python y así crear aplicaciones o sitios web que utilicen el database para arrojar resultados. \n",
    "### Conforme la base de datos de TLC se renueve, nuestro `Data Warehouse` se actualizará automáticamente mediante nuestra función `Lambda` que tomará los nuevos registros de viajes y los almacenará en el `Data Warehouse`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Entrenamiento del Modelo de Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para realizar el entrenamiento se optó por adaptar el código creado en local, a las herramientas que nos brinda AWS. Para ello se creó un notebook en [Amazon SageMaker](https://aws.amazon.com/es/sagemaker/) y se utilizó el servicio de [Amazon S3](https://aws.amazon.com/es/s3/) para almacenar los datos de entrenamiento y los modelos generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import boto3\n",
    "import warnings\n",
    "\n",
    "# Ignorar todas las advertencias\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuración de credenciales para acceder a S3\n",
    "region_name = 'tu_region'\n",
    "aws_access_key_id = 'tu_access_key'\n",
    "aws_secret_access_key = 'tu_secret_key'\n",
    "\n",
    "# Cambia la ruta de S3 para cargar el DataFrame\n",
    "s3_path = 's3://tu_bucket/tu_archivo'\n",
    "df = pd.read_parquet(s3_path)\n",
    "\n",
    "# Función para agregar la columna 'pBoroughID'\n",
    "def addBoroughID(df):\n",
    "    df['pBoroughID'] = df['pickup_borough'].map({'Manhattan': 1, 'Brooklyn': 2, 'Queens': 3, 'Bronx': 4, 'Staten Island': 5}).fillna(6)\n",
    "    return df\n",
    "\n",
    "df = addBoroughID(df)\n",
    "\n",
    "# Obtener mes, día de la semana, hora y día de la semana\n",
    "df['month'] = df['pickup_datetime'].dt.month\n",
    "df['dayofweek'] = df['pickup_datetime'].dt.dayofweek\n",
    "df['hour'] = df['pickup_datetime'].dt.hour\n",
    "\n",
    "# Agrupamos por localización, hora y día de la semana\n",
    "df_ml = df.groupby(['hour', 'pBoroughID', 'dayofweek']).size().reset_index(name='demand')\n",
    "\n",
    "# Creamos el porcentaje de Demanda\n",
    "df_ml['demand'] = df_ml['demand'].apply(lambda x: (x / df_ml['demand'].max()))\n",
    "df_ml['demand'] = round(df_ml['demand'] * 100, 2)\n",
    "\n",
    "# Seleccionar características y variable objetivo\n",
    "features = ['pBoroughID', 'dayofweek', 'hour']\n",
    "target = 'demand'\n",
    "\n",
    "# Plantear las variables dependientes e independientes\n",
    "X = df_ml[features]\n",
    "y = df_ml[target]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Inicializar y entrenar el modelo XGBRegressor\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "print(f'R-squared (R2): {r2}')\n",
    "\n",
    "# Guardar el modelo en un archivo pickle\n",
    "joblib.dump(xgb_model, 'tu_modelo_entrenado')\n",
    "\n",
    "# Guardar el modelo en S3\n",
    "s3_model_path = 's3://tu_bucket/tu_carpeta/tu_modelo_entrenado'\n",
    "boto3.client('s3', region_name=region_name, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key).upload_file('tu_modelo_entrenado', 'tu_bucket', 'tu_carpeta/tu_modelo_entrenado')\n",
    "print('Modelo entrenado y guardado en la ruta s3://tu_bucket/tu_carpeta/tu_modelo_entrenado')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
